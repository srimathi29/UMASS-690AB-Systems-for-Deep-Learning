# -*- coding: utf-8 -*-
"""COMPSCI 690AB HW1 Framework.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W6UFtXH3jaUqwe5-DjxwSEKaaCKQqQiF

# **COMPSCI 690AB Spring 2024: HW 1 Frameworks**

This colab notebook provides code and a framework for HW1 Computation Frameworks. We explore how to train a neural network with PyTorch. We will also use Pytorch profiler to profile the training and inference process. You can work out your solutions here.

You have two weeks to work on this assignment. You should submit this notebook on Gradescope no later than **March 5th**.

## Goals

In this assignment, you will have hands-on experience on how modern computation frameworks designed for DNN work. We will use Pytorch framework to express a DNN model and train the model to reach high accuracy on CIFAR10 dataset. The goals of this assignment are as follows:
- Understand the basic concept of programming framework, including but not limited to **compute primitives**, e.g., nn.conv2d, **automatic differentiation**, etc.
- Understand the **performance metrics** during training and inference, including time and memory consumption.

## Contents
There are two main sections in this lab and 7 questions in total:
- Section 1: setup environment and model training (Q1-Q2)
- Section 2: profile model execution time and memory consumption (Q3-Q7)

The total number of points are 70.

# Section 1: Setup

We will first install a few packages that will be used in this tutorial:
"""

!pip install torchprofile 1>/dev/null
!pip install gdown

"""We will then import a few libraries:"""

import copy
import random
from collections import OrderedDict, defaultdict

import numpy as np
import torch
from matplotlib import pyplot as plt
from torch import nn
from torch.optim import *
from torch.optim.lr_scheduler import *
from torch.utils.data import DataLoader
from torchprofile import profile_macs
from torchvision.datasets import *
from torchvision.transforms import *
from tqdm.auto import tqdm

from torch.profiler import profile, record_function, ProfilerActivity

assert torch.cuda.is_available(), \
"The current runtime does not have CUDA support." \
"Please go to menu bar (Runtime - Change runtime type) and select GPU"

"""To ensure the reproducibility, we will control the seed of random generators:"""

random.seed(0)
np.random.seed(0)
torch.manual_seed(0)

def download_url(url, model_dir='.', overwrite=False):
    import os, sys, ssl, gdown
    # from urllib.request import urlretrieve
    ssl._create_default_https_context = ssl._create_unverified_context
    target_dir = url.split('/')[-1]
    model_dir = os.path.expanduser(model_dir)
    try:
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
        model_dir = os.path.join(model_dir, target_dir)
        cached_file = model_dir
        if not os.path.exists(cached_file) or overwrite:
            sys.stderr.write('Downloading: "{}" to {}\n'.format(url, cached_file))
            # urlretrieve(url, cached_file)
            gdown.download(url, cached_file, quiet=False)
        return cached_file
    except Exception as e:
        # remove lock file so download can be executed next time.
        os.remove(os.path.join(model_dir, 'download.lock'))
        sys.stderr.write('Failed to download from url %s' % url + '\n' + str(e) + '\n')
        return None

"""## Prepare Data

In this HW, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of
size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.
"""

transforms = {
  "train": Compose([
    RandomCrop(32, padding=4),
    RandomHorizontalFlip(),
    ToTensor(),
  ]),
  "test": ToTensor(),
}

dataset = {}
for split in ["train", "test"]:
  dataset[split] = CIFAR10(
    root="data/cifar10",
    train=(split == "train"),
    download=True,
    transform=transforms[split],
  )

"""We can visualize a few images in the dataset and their corresponding class labels:"""

samples = [[] for _ in range(10)]
for image, label in dataset["test"]:
  if len(samples[label]) < 4:
    samples[label].append(image)

plt.figure(figsize=(20, 9))
for index in range(40):
  label = index % 10
  image = samples[label][index // 10]

  # Convert from CHW to HWC for visualization
  image = image.permute(1, 2, 0)

  # Convert from class index to class name
  label = dataset["test"].classes[label]

  # Visualize the image
  plt.subplot(4, 10, index + 1)
  plt.imshow(image)
  plt.title(label)
  plt.axis("off")
plt.show()

"""To train a neural network, we will need to feed data in batches. We create data loaders with batch size of 512:"""

dataflow = {}
for split in ['train', 'test']:
  dataflow[split] = DataLoader(
    dataset[split],
    batch_size=512,
    shuffle=(split == 'train'),
    num_workers=0,
    pin_memory=True,
  )

"""We can print the data type and shape from the training data loader:"""

for inputs, targets in dataflow["train"]:
  print("[inputs] dtype: {}, shape: {}".format(inputs.dtype, inputs.shape))
  print("[targets] dtype: {}, shape: {}".format(targets.dtype, targets.shape))
  break

"""## Define Model

In this HW, we will use a variant of [VGG-11](https://arxiv.org/abs/1409.1556) (with fewer downsamples and a smaller classifier) as our model.
"""

class VGG(nn.Module):
  ARCH = [64, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']

  def __init__(self) -> None:
    super().__init__()

    layers = []
    counts = defaultdict(int)

    def add(name: str, layer: nn.Module) -> None:
      layers.append((f"{name}{counts[name]}", layer))
      counts[name] += 1

    in_channels = 3
    for x in self.ARCH:
      if x != 'M':
        # conv-bn-relu
        add("conv", nn.Conv2d(in_channels, x, 3, padding=1, bias=False))
        add("bn", nn.BatchNorm2d(x))
        add("relu", nn.ReLU(True))
        in_channels = x
      else:
        # maxpool
        add("pool", nn.MaxPool2d(2))

    self.backbone = nn.Sequential(OrderedDict(layers))
    self.classifier = nn.Linear(512, 10)

  def forward(self, x: torch.Tensor) -> torch.Tensor:
    # backbone: [N, 3, 32, 32] => [N, 512, 2, 2]
    x = self.backbone(x)

    # avgpool: [N, 512, 2, 2] => [N, 512]
    x = x.mean([2, 3])

    # classifier: [N, 512] => [N, 10]
    x = self.classifier(x)
    return x

model = VGG().cuda()

"""Its backbone is composed of eight `conv-bn-relu` blocks interleaved with four `maxpool`'s to downsample the feature map by 2^4 = 16 times:"""

print(model.backbone)

"""After the feature map is pooled, its classifier predicts the final output with a linear layer:"""

print(model.classifier)

"""As this course focuses on system performance (i.e., latency, throughput), we will then inspect its model size and (theoretical) computation cost.

* The model size can be estimated by the number of trainable parameters:
"""

num_params = 0
for param in model.parameters():
  if param.requires_grad:
    num_params += param.numel()
print("#Params:", num_params)

"""### Question 1: (10 points)
Explain why the total number of trainable parameters is 9228362. You should analyze how many trainable parameters are in each NN layer.

**Hint:**
- There are a total 8 conv layers, 8 BN layers, and 1 FC layer. Each layer has their trainable parameters.
- Each BN layer has two sets of trainable parametesr, the scale vector and the bias vector. You can refer to this [Pytorch Banchnorm2D](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) API to figure out the mechanism.

**Your answer:**
convolutional layer 0:  3 × 64 × 3 × 3 = 1728

convolutional layer 1: 64 × 128 × 3 × 3 = 73728

convolutional layer 2:  128 × 256 × 3 × 3=294912

convolutional layer 3: 256 × 256 × 3 × 3=589824

convolutional layer 4: 256 × 512 × 3 × 3=1179648

convolutional layer 5: 512 × 512 × 3 × 3=2359296

convolutional layer 6: 512 × 512 × 3 × 3=2359296

convolutional layer 7: 512 × 512 × 3 × 3=2359296

Batch Normalization Layers: Each layer has two sets of trainable parameters and 8 Batch Normalization layers. 2×8=16 trainable parameter
The fully connected layer has 512×10=5120 trainable parameters.
Therefore when we add them up, total = 9228362

* The computation cost for inference can be estimated by the number of [multiply–accumulate operations (MACs)](https://en.wikipedia.org/wiki/Multiply–accumulate_operation) using [TorchProfile](https://github.com/zhijian-liu/torchprofile):
"""

num_macs = profile_macs(model, torch.zeros(1, 3, 32, 32).cuda())
print("#MACs:", num_macs)

"""This model has 9.2M parameters and requires 606M MACs for inference.

### Question 2 (10 points):
Are the total #MACs equally distributed across the NN? If not, which layer has the most number of MAC? Identify the layer ID as one of {conv0, conv2,..., conv7}.

**Hints**:
- The MACs of a CONV layer = output size x kernel shape x #output channels, where output size = H x W, kernel shape = #input_channels x kernel size
- For example, for conv0, its setting is: `Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)...)`. This means its MACs = 32 x 32 (output size) x 3 x 3 x 3 (kernel shape) x 64 (#output channels)
- In our model, all the conv layers use a stride of 1 and padding of 1. So the output size (H, W) is exactly the same as the input activation map size.

**Your answer:**
Calculating MACs distribution across all conv layers we see that the distribution is not equal.

(conv0): 1769472

(conv1): 75497472

(conv2): 301989888

(conv3): 603979776

(conv4): 1207959552

(conv5): 2415919104

(conv6): 2415919104

(conv7): 2415919104

The conv5, conv6, conv7 these three layers have the most number of MAC.

## Define Optimizer

As we are working on a classification problem, we will apply [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) as our loss function to optimize the model:
"""

criterion = nn.CrossEntropyLoss()

"""Optimization will be carried out using [stochastic gradient descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) with [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum):"""

optimizer = SGD(
  model.parameters(),
  lr=0.4,
  momentum=0.9,
  weight_decay=5e-4,
)

"""The learning rate will be modulated using the following scheduler (which is adapted from [this blog series](https://myrtle.ai/learn/how-to-train-your-resnet/)):"""

num_epochs = 20
steps_per_epoch = len(dataflow["train"])

# Define the piecewise linear scheduler
lr_lambda = lambda step: np.interp(
  [step / steps_per_epoch],
  [0, num_epochs * 0.3, num_epochs],
  [0, 1, 0]
)[0]

# Visualize the learning rate schedule
steps = np.arange(steps_per_epoch * num_epochs)
plt.plot(steps, [lr_lambda(step) * 0.4 for step in steps])
plt.xlabel("Number of Steps")
plt.ylabel("Learning Rate")
plt.grid("on")
plt.show()

scheduler = LambdaLR(optimizer, lr_lambda)

"""## Training

We first define the training function that optimizes the model for one epoch (*i.e.*, a pass over the training set):
"""

def train(
  model: nn.Module,
  dataflow: DataLoader,
  criterion: nn.Module,
  optimizer: Optimizer,
  scheduler: LambdaLR,
) -> None:
  model.train()

  for inputs, targets in tqdm(dataflow, desc='train', leave=False):
    # Move the data from CPU to GPU
    inputs = inputs.cuda()
    targets = targets.cuda()

    # Reset the gradients (from the last iteration)
    optimizer.zero_grad()

    # Forward inference
    outputs = model(inputs)
    loss = criterion(outputs, targets)

    # Backward propagation
    loss.backward()

    # Update optimizer and LR scheduler
    optimizer.step()
    scheduler.step()

"""We then define the evaluation function that calculates the metric (*i.e.*, accuracy in our case) on the test set:"""

@torch.inference_mode()
def evaluate(
  model: nn.Module,
  dataflow: DataLoader
) -> float:
  model.eval()

  num_samples = 0
  num_correct = 0

  for inputs, targets in tqdm(dataflow, desc="eval", leave=False):
    # Move the data from CPU to GPU
    inputs = inputs.cuda()
    targets = targets.cuda()

    # Inference
    outputs = model(inputs)

    # Convert logits to class indices
    outputs = outputs.argmax(dim=1)

    # Update metrics
    num_samples += targets.size(0)
    num_correct += (outputs == targets).sum()

  return (num_correct / num_samples * 100).item()

"""With training and evaluation functions, we can finally start training the model! This will take around 10 minutes."""

def load_pretrained_model(model):
  checkpoint_url="https://drive.google.com/uc?id=1PQ67cwyjdV2v493jyX686vO-VAGobnkv"
  checkpoint = torch.load(download_url(checkpoint_url), map_location="cpu")
  print(f"=> loading checkpoint '{checkpoint_url}'")
  model.load_state_dict(checkpoint['state_dict'])
  return model

# If you don't want to train the model,
# set the flag to be False so that a pre-trained model will be downloaded.
TRAINING_FLAG = True
if TRAINING_FLAG:
  print('Training the model...')
  for epoch in tqdm(range(1, num_epochs + 1)):
    train(model, dataflow["train"], criterion, optimizer, scheduler)
    accuracy = evaluate(model, dataflow["test"])
    print(f'    Epoch {epoch+1} Accuracy {accuracy:.2f}%')
else:
  model = load_pretrained_model(model)
  accuracy = evaluate(model, dataflow["test"])
  print(f'Use downloaded model, Accuracy {accuracy:.2f}%')

"""If everything goes well, your trained model should be able to achieve >92.5\% of accuracy!

## Visualizing Prediction Results

We can visualize the model's prediction to see how the model truly performs:
"""

plt.figure(figsize=(20, 10))
for index in range(40):
  image, label = dataset["test"][index]

  # Model inference
  model.eval()
  with torch.inference_mode():
    pred = model(image.unsqueeze(dim=0).cuda())
    pred = pred.argmax(dim=1)

  # Convert from CHW to HWC for visualization
  image = image.permute(1, 2, 0)

  # Convert from class indices to class names
  pred = dataset["test"].classes[pred]
  label = dataset["test"].classes[label]

  # Visualize the image
  plt.subplot(4, 10, index + 1)
  plt.imshow(image)
  plt.title(f"pred: {pred}" + "\n" + f"label: {label}")
  plt.axis("off")
plt.show()

"""# Section 2: Using profiler to analyze execution time and memory consumption

PyTorch includes a simple profiler API that is useful when user needs to determine the most expensive operators in the model.

In this section, we will use [pytorch profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html#introduction) to analyze model performance

## Profile time consuption with batch size = 1
PyTorch profiler is enabled through the context manager and accepts a number of parameters, some of the most useful are:
- `activities` - a list of activities to profile (`ProfilerActivity.CPU`, `ProfilerActivity.CUDA`)
- `record_shapes` - whether to record shapes of the operator inputs;
- `profile_memory` - whether to report amount of memory consumed by model Tensors;

Below, we profied the execution time of the model using batch size = 1. Inference happens on the GPU.
"""

# Use the 5-th test image
model.eval()
model.cuda()
with torch.inference_mode():
  for index in range(5):
    test_input = torch.randn([1, 3, 32, 32])
    with profile(activities=[
          ProfilerActivity.CPU]) as prof:
      with record_function("model_inference"):
          pred = model(test_input.cuda())

# Below, we visualize the profiled results. Let's group by operator name.
# We show the top 10 operators used during inference that have most of the cpu_time_total.
print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))

"""Note the difference between self cpu time and cpu time - operators can call other operators, self cpu time excludes time spent in children operator calls, while total cpu time includes it.




The entry `aten::cudnn_convolution` in the profiling summary indicates a convolution operation that is implemented using the cuDNN library. The `aten::cudnn_convolution` is not a GPU kernel itself; rather, it represents a high-level PyTorch operation that utilizes the cuDNN library to perform convolution on the GPU.


CuDNN (CUDA Deep Neural Network library) is a GPU-accelerated library developed by NVIDIA specifically for deep neural networks. It provides highly optimized implementations of various deep learning operations, including convolutions. When you see `aten::cudnn_convolution` in the profiling results, it means that PyTorch is using cuDNN to execute the convolution operation on the GPU.

Under the hood, cuDNN may use GPU kernels to perform the actual computation, but the `aten::cudnn_convolution` entry itself is a PyTorch operation that **abstracts away the low-level details of the GPU kernel implementations**.

### Question 3: (10 points)
From the above profiling results, what is the performance bottleneck of the inference? Why? Performance bottleneck refers to the part of a program that takes the longest time to finish.

**Hints**
- GPU inference has two main steps: (1) transfer the input data from CPU to GPU (mapped to the `aten::to` operator), and (2) execute the model on GPU to produce output.

**Your answer:** Based on the profiling results **atten::to** operetor seems to be the performance bottleneck, this operator is responsible for tranfer of input data from CPU to GPU. This operator has high self CPU time percentage.This is likey because, transfer of data from CPU to GPU involves very large data and it could be a relatively slow process.

## Profile time consumption with batch size = 1024
"""

model.eval()
model.cuda()
with torch.inference_mode():
  for _ in range(5):
    inputs = torch.randn([1024, 3, 32, 32])
    with profile(activities=[
          ProfilerActivity.CPU]) as prof:
      with record_function("model_inference"):
          pred = model(inputs.cuda())

# Visualize the profiled execution time
print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))

"""### Question 4: (10 points)
Although we increase the batch size to 1024, which is 1024 times larger than when the batch size is one. The total inference time doesn't increase by 1024. Why?

**Your answer:** The increase in total inference time is not linearly proportional to the increase in batch size because of the efficiency gained through parallel processing. When the batch size is increased, GPU processes multiple inputs simultaneously. Hence, even though the batch size is 1024, the total inference time isn't 1024.

### Question 5: (10 points)
Now we compare two inference systems where one system uses batch size = 1 and the other system uses batch size = 1024. Which system has the smallest latency per input? and which system has the higher throughput (inputs per second)? Why?

**Your answer:**

Batch Size 1
Latency (Cpu Avg time): 2.539 ms \\
Throughput: (batch size)/(time per batch) = 1/2.539ms = 424 inferences per second \\
Batch Size 1024 \\
Latency (Cpu Avg time): 116.451 ms \\
Inference Throughput : (batch size)/(time per batch) = 1024/116.451ms = 8793  inferences per second \\
Comparing the above results \\
batch size 1 gave the smallest latency per input \\
batch size 1024 has highest throughput \\


Due to high parallelization leverage the system with batch size 1024 gave high throughput. This is because it processes multiple parameters/calculations at once and efficiently, compared with batch 1 it just calculates 1 thing.

## Now We profile time consumption on CPU
"""

# Use the 5-th test image
model.eval()
model.to("cpu")
with torch.inference_mode():
  for index in range(5):
    test_input = torch.randn([1, 3, 32, 32])
    with profile(activities=[
          ProfilerActivity.CPU]) as prof:
      with record_function("model_inference"):
          pred = model(test_input)

# Below, we visualize the profiled results. Let's group by operator name.
# We show the top 10 operators used during inference that have most of the cpu_time_total.
print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))

"""Note that now PyTorch uses `aten::mkldnn_convolution` for the conv2d operation instead of `aten::cudnn_convolution` (when GPU is used).

### Question 6: (10 points)
With cpu-based inference, what is the peformance bottleneck of the inference? and why?

**Your answer:** The performance bottleneck seems to be the **atten::mkldnn_convolution** operation, which has the highest CPU time. This is probably because of the differences in computational efficiency, hardware acceleration, memory bandwidth, and parallelism between CPUs and GPUs.

## Profile memory consumption

PyTorch profiler can also show the amount of memory (used by the model's tensors) that was allocated (or released) during the execution of the model's operators. To enable memory profiling functionality pass `profile_memory=True`.

Below, we use the profiler to understand the memory usage across time during training. The source code snippets are adapted from [this blog of PyTorch](https://pytorch.org/blog/understanding-gpu-memory-1/). The Memory Profiler automatically generates categories based on the graph of tensor operations recorded during profiling.
"""

# We will create a new model
model2 = VGG().cuda()
model2.train()

# We create fake inputs and targets
inputs = torch.rand(512, 3, 32, 32).cuda()
labels = torch.rand_like(model2(inputs)).cuda()

# set up a plain SGD optimizer for the new model
optimizer2 = SGD(
  model2.parameters(),
  lr=0.4,
  momentum=0.9,
  weight_decay=5e-4,
)


with torch.profiler.profile(
       activities=[
           torch.profiler.ProfilerActivity.CPU,
           torch.profiler.ProfilerActivity.CUDA,
       ],
       schedule=torch.profiler.schedule(wait=3, warmup=1, active=7, repeat=1),
       record_shapes=True,
       profile_memory=True,
       with_stack=True,) as prof:
    for _ in range(6):
        prof.step()
        with record_function("## forward ##"):
            pred = model2(inputs)
        with record_function("## backward ##"):
            criterion(pred, labels).backward()
        with record_function("## optimizer ##"):
            optimizer2.step()
            optimizer2.zero_grad()

# Construct the memory timeline HTML plot.
prof.export_memory_timeline(f"memory_profile.html", device="cuda:0")

"""In this Memory Timeline collected using the Memory Profiler, the memory consumed by the parameters are colored in green while the memory consumed by the activations are colored in red. Gradients are colored in blue.

#### Question 7: (10 points)

The memory timeline reports the memory consumption for three training iterations. The memory consumed by the model parameters is constant as the model stays in the GPU memory. However, the activation maps' memory consumption goes up and down within one iteration.

Can you explain the pattern of memory consumption from activations? Why does it go up and down within one iteration?

**Your answer:** The pattern of memory consumption from activations is because, memory is allocated for activation maps during the forward pass. During the backward pass, memory is again allocated for storing the gradient of the activation maps. Once this is complete, the memory allocated for activation maps and gradient is released. This cycle is repeated across training iterations leading to this pattern.
"""

















































































































































































